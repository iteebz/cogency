#!/usr/bin/env python3
"""
COMPREHENSIVE LANGGRAPH INTEGRATION AUDIT
Tests all the bullshit from the audit prompt
"""
import asyncio
import inspect
from cogency import Agent, WeatherTool, CalculatorTool
from cogency.nodes import think, plan, act, reflect, respond
from cogency.llm import OpenAILLM, auto_detect_llm
from cogency.flow import Flow
from cogency.types import AgentState
from cogency.context import Context

async def test_1_node_return_types():
    """Test 1: Node Return Types - FAIL if returns async generator"""
    print("ğŸ” TEST 1: Node Return Types")
    
    # Mock LLM for testing
    class MockLLM:
        async def invoke(self, messages, **kwargs):
            return "test thinking response"
        async def ainvoke(self, messages, **kwargs):
            return await self.invoke(messages, **kwargs)
    
    mock_llm = MockLLM()
    state = {
        "context": Context(current_input="test query"),
        "execution_trace": None
    }
    
    # Test each node
    nodes = [
        ("think", lambda: think(state, llm=mock_llm)),
        ("plan", lambda: plan(state, llm=mock_llm, tools=[], prompt_fragments={})),
        ("act", lambda: act(state, tools=[])),
        ("reflect", lambda: reflect(state, llm=mock_llm)),
        ("respond", lambda: respond(state, llm=mock_llm, prompt_fragments={}))
    ]
    
    for node_name, node_func in nodes:
        result = await node_func()
        assert isinstance(result, dict), f"âŒ {node_name} returned {type(result)}, expected dict"
        assert "context" in result, f"âŒ {node_name} missing 'context' in result"
        print(f"  âœ… {node_name} returns dict")
    
    print("  âœ… ALL NODES RETURN DICTS\n")


async def test_2_llm_interface():
    """Test 2: LLM Interface Compatibility"""
    print("ğŸ” TEST 2: LLM Interface Compatibility")
    
    llm = auto_detect_llm()
    
    # Test both interfaces exist
    assert hasattr(llm, "invoke"), "âŒ LLM missing invoke() method"
    assert hasattr(llm, "ainvoke"), "âŒ LLM missing ainvoke() method"
    
    # Test both are async
    assert inspect.iscoroutinefunction(llm.invoke), "âŒ invoke() is not async"
    assert inspect.iscoroutinefunction(llm.ainvoke), "âŒ ainvoke() is not async"
    
    print("  âœ… LLM has both invoke() and ainvoke()")
    print("  âœ… Both methods are async")
    print("  âœ… LLM INTERFACE COMPATIBLE\n")


async def test_3_workflow_execution():
    """Test 3: LangGraph Integration - FAIL if gets coroutine objects"""
    print("ğŸ” TEST 3: LangGraph Integration")
    
    llm = auto_detect_llm()
    tools = [WeatherTool(), CalculatorTool()]
    flow = Flow(llm, tools)
    
    state = {
        "context": Context(current_input="test query"),
        "execution_trace": None
    }
    
    # Test workflow executes without coroutine errors
    try:
        result = await flow.workflow.ainvoke(state)
        assert isinstance(result, dict), f"âŒ Workflow returned {type(result)}, expected dict"
        assert "context" in result, "âŒ Workflow result missing 'context'"
        print("  âœ… Workflow executes without coroutine errors")
        print("  âœ… Workflow returns proper dict state")
    except Exception as e:
        if "coroutine" in str(e).lower():
            print(f"  âŒ COROUTINE ERROR: {e}")
            raise
        else:
            raise
    
    print("  âœ… LANGGRAPH INTEGRATION WORKS\n")


async def test_4_streaming_capability():
    """Test 4: Streaming Works"""
    print("ğŸ” TEST 4: Streaming Capability")
    
    agent = Agent("test_agent", tools=[WeatherTool()])
    
    # Test streaming produces output
    result = await agent.stream("What's 2+2?")
    assert isinstance(result, str), f"âŒ Stream returned {type(result)}, expected str"
    assert len(result) > 0, "âŒ Stream returned empty result"
    
    print("  âœ… Streaming produces output")
    print("  âœ… Returns proper string response")
    print("  âœ… STREAMING WORKS\n")


async def test_5_complex_workflow():
    """Test 5: Complex Example - Multi-step with tools"""
    print("ğŸ” TEST 5: Complex Workflow")
    
    agent = Agent("complex_agent", tools=[WeatherTool(), CalculatorTool()])
    
    # Test complex query that requires multiple steps
    result = await agent.stream("What's the weather in London and what's 25 * 4?")
    assert isinstance(result, str), f"âŒ Complex workflow returned {type(result)}, expected str"
    assert len(result) > 0, "âŒ Complex workflow returned empty result"
    
    print("  âœ… Complex workflow executes")
    print("  âœ… Handles multiple tools")
    print("  âœ… COMPLEX WORKFLOW WORKS\n")


async def test_6_edge_cases():
    """Test 6: Edge Cases"""
    print("ğŸ” TEST 6: Edge Cases")
    
    agent = Agent("edge_case_agent")
    
    # Test simple query without tools
    result = await agent.stream("Hello, how are you?")
    assert isinstance(result, str), f"âŒ No-tools query returned {type(result)}, expected str"
    assert len(result) > 0, "âŒ No-tools query returned empty result"
    
    print("  âœ… Works without tools")
    print("  âœ… Handles simple queries")
    print("  âœ… EDGE CASES WORK\n")


async def main():
    """Run comprehensive LangGraph audit"""
    print("ğŸš€ COMPREHENSIVE LANGGRAPH INTEGRATION AUDIT")
    print("=" * 60)
    
    try:
        await test_1_node_return_types()
        await test_2_llm_interface()
        await test_3_workflow_execution()
        await test_4_streaming_capability()
        await test_5_complex_workflow()
        await test_6_edge_cases()
        
        print("ğŸ‰ ALL TESTS PASSED - LANGGRAPH INTEGRATION VERIFIED")
        print("âœ… Nodes return dicts")
        print("âœ… LLM has proper interface")
        print("âœ… Flow uses pure orchestration")
        print("âœ… Streaming works")
        print("âœ… Complex workflows work")
        print("âœ… Zero compatibility issues")
        
    except Exception as e:
        print(f"âŒ AUDIT FAILED: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())